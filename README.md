# AML Adversarial Attacks (Colab Project)

This repository hosts the notebook **Project_AML_Adversarial_Attacks.ipynb**, showcasing experiments on adversarial attacks for applied machine learning.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dishachak/aml-adversarial-attacks/blob/main/notebooks/Project_AML_Adversarial_Attacks.ipynb)
<!-- After pushing to GitHub, replace <your-username> and ensure the path matches main/notebooks/... -->

## What’s inside
- `notebooks/Project_AML_Adversarial_Attacks.ipynb` — main Colab notebook.
- `requirements.txt` — auto-detected dependencies from imports (edit if needed).
- `figures/` — put images for the README (results/plots).
- `.gitignore` — ignores common Python/Colab artifacts.
- `LICENSE` — MIT (edit as desired).

## Tech stack (edit)
- Python, Google Colab
- Likely libraries: numpy, scipy, matplotlib, IPython, seaborn, tqdm, torch, torchvision, pytorch_lightning, urllib, tabulate, tensorflow, tensorflow_datasets, pandas

## Getting started
```bash
git clone https://github.com/<your-username>/aml-adversarial-attacks.git
cd aml-adversarial-attacks
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

Open the notebook locally with Jupyter or click the Colab badge above once the repo is public.

## Results & discussion
Add 2–5 bullet points summarizing what you implemented, attack methods, datasets, and key metrics.

## Dataset(s)
- List dataset names/links and licenses.

## Future work
- Brief bullets for next experiments.
